# Machine Learning Experiments
Compilation of papers, blog posts, repositories, and other experiments from around the Internet.
I put this together mostly to reduce the amount of open tabs I have in Chrome and so that I can
visit whenever I have free time to experiment.

-------------------------------------------------------------------------------
## Machine Learning for Software Engineers
A multi-month study plan for going from a mobile developer to machine learning engineer.

https://github.com/ZuzooVn/machine-learning-for-software-engineers
-------------------------------------------------------------------------------
## Oxford Deep NLP 2017 course
This repository contains the lecture slides and course description for the Deep Natural Language Processing course offered in Hilary Term 2017 at the University of Oxford.

This is an advanced course on natural language processing. Automatically processing natural language inputs and producing language outputs is a key component of Artificial General Intelligence. The ambiguities and noise inherent in human communication render traditional symbolic AI techniques ineffective for representing and analysing language data. Recently statistical techniques based on neural networks have achieved a number of remarkable successes in natural language processing leading to a great deal of commercial and academic interest in the field

This is an applied course focussing on recent advances in analysing and generating speech and text using recurrent neural networks. We introduce the mathematical definitions of the relevant machine learning models and derive their associated optimisation algorithms. The course covers a range of applications of neural networks in NLP including analysing latent dimensions in text, transcribing speech to text, translating between languages, and answering questions. These topics are organised into three high level themes forming a progression from understanding the use of neural networks for sequential language modelling, to understanding their use as conditional language models for transduction tasks, and finally to approaches employing these techniques in combination with other mechanisms for advanced applications. Throughout the course the practical implementation of such models on CPU and GPU hardware is also discussed.

This course is organised by Phil Blunsom and delivered in partnership with the DeepMind Natural Language Research Group.

https://github.com/oxford-cs-deepnlp-2017/lectures
-------------------------------------------------------------------------------
## DeepMind’s PathNet: A Modular Deep Learning Architecture for AGI
PathNet is a new Modular Deep Learning (DL) architecture, brought to you by who else but DeepMind, that highlights the latest trend in DL research to meld Modular Deep Learning, Meta-Learning and Reinforcement Learning into a solution that leads to more capable DL systems. A January 20th, 2017 submitted Arxiv paper “PathNet: Evolution Channels Gradient Descent in Super Neural Networks” (Fernando et. al) has in its abstract the following interesting description of the work:

>For artificial general intelligence (AGI) it would be efficient if multiple users trained the same giant neural network, permitting parameter reuse, without catastrophic forgetting. PathNet is a first step in this direction. It is a neural network algorithm that uses agents embedded in the neural network whose task is to discover which parts of the network to re-use for new tasks.

https://medium.com/intuitionmachine/pathnet-a-modular-deep-learning-architecture-for-agi-5302fcf53273
-------------------------------------------------------------------------------
## NSynth: Neural Audio Synthesis
One of the goals of Magenta is to use machine learning to develop new avenues of human expression. And so today we are proud to announce NSynth (Neural Synthesizer), a novel approach to music synthesis designed to aid the creative process.

Unlike a traditional synthesizer which generates audio from hand-designed components like oscillators and wavetables, NSynth uses deep neural networks to generate sounds at the level of individual samples. Learning directly from data, NSynth provides artists with intuitive control over timbre and dynamics and the ability to explore new sounds that would be difficult or impossible to produce with a hand-tuned synthesizer.

The acoustic qualities of the learned instrument depend on both the model used and the available training data, so we are delighted to release improvements to both:

- A dataset of musical notes an order of magnitude larger than other publicly available corpora.
- A novel WaveNet-style autoencoder model that learns codes that meaningfully represent the space of instrument sounds.

https://magenta.tensorflow.org/nsynth
-------------------------------------------------------------------------------
## Market Vectors
In many NLP problems we end up taking a sequence and encoding it into a single fixed size representation, then decoding that representation into another sequence. For example, we might tag entities in the text, translate from English to French or convert audio frequencies to text. There is a torrent of work coming out in these areas and a lot of the results are achieving state of the art performance.

In my mind the biggest difference between the NLP and financial analysis is that language has some guarantee of structure, it’s just that the rules of the structure are vague. Markets, on the other hand, don’t come with a promise of a learnable structure, that such a structure exists is the assumption that this project would prove or disprove (rather it might prove or disprove if I can find that structure).

Assuming the structure is there, the idea of summarizing the current state of the market in the same way we encode the semantics of a paragraph seems plausible to me.

https://github.com/talolard/MarketVectors/blob/master/preparedata.ipynb
-------------------------------------------------------------------------------
## A Visual and Interactive Guide to the Basics of Neural Networks
I’m not a machine learning expert. I’m a software engineer by training and I’ve had little interaction with AI. I had always wanted to delve deeper into machine learning, but never really found my “in”. That’s why when Google open sourced TensorFlow in November 2015, I got super excited and knew it was time to jump in and start the learning journey. Not to sound dramatic, but to me, it actually felt kind of like Prometheus handing down fire to mankind from the Mount Olympus of machine learning. In the back of my head was the idea that the entire field of Big Data and technologies like Hadoop were vastly accelerated when Google researchers released their Map Reduce paper. This time it’s not a paper – it’s the actual software they use internally after years and years of evolution.

So I started learning what I can about the basics of the topic, and saw the need for gentler resources for people with no experience in the field. This is my attempt at that.

https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/
-------------------------------------------------------------------------------
## Project Description
Description

https://karpathy.github.io/2016/05/31/rl/
-------------------------------------------------------------------------------
## Project Description
Description

http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/
-------------------------------------------------------------------------------
## Project Description
Description

https://medium.freecodecamp.com/simple-chess-ai-step-by-step-1d55a9266977
-------------------------------------------------------------------------------
## Project Description
Description

https://github.com/luanfujun/deep-photo-styletransfer?utm_content=buffer39dd6&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer
-------------------------------------------------------------------------------
## Project Description
Description

https://karpathy.github.io/2015/05/21/rnn-effectiveness/
-------------------------------------------------------------------------------
## Project Description
Description

https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471
-------------------------------------------------------------------------------
## Project Description
Description

https://hackernoon.com/learning-ai-if-you-suck-at-math-p5-deep-learning-and-convolutional-neural-nets-in-plain-english-cda79679bbe3
-------------------------------------------------------------------------------
## Project Description
Description

https://www.oreilly.com/learning/caption-this-with-tensorflow
-------------------------------------------------------------------------------
## Project Description
Description

https://medium.freecodecamp.com/big-picture-machine-learning-classifying-text-with-neural-networks-and-tensorflow-d94036ac2274
-------------------------------------------------------------------------------
## Project Description
Description

https://devblogs.nvidia.com/parallelforall/recursive-neural-networks-pytorch/
-------------------------------------------------------------------------------
## Project Description
Description

https://github.com/mrzl/ofxDarknet
-------------------------------------------------------------------------------
## Project Description
Description

https://arxiv.org/ftp/arxiv/papers/1704/1704.01568.pdf
-------------------------------------------------------------------------------
## Project Description
Description

https://blogs.dropbox.com/tech/2017/04/creating-a-modern-ocr-pipeline-using-computer-vision-and-deep-learning/
-------------------------------------------------------------------------------
## Project Description
Description

https://github.com/Luubra/EmojiIntelligence?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=revue
-------------------------------------------------------------------------------
## Project Description
Description

https://github.com/Heumi/BEGAN-tensorflow?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=revue
-------------------------------------------------------------------------------
## Project Description
Description

https://medium.com/@taposhdr/medical-image-analysis-with-deep-learning-i-23d518abf531
-------------------------------------------------------------------------------
## Project Description
Description

https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python?utm_content=buffer4c85c&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer
-------------------------------------------------------------------------------
## Project Description
Description

https://github.com/aaron-xichen/pytorch-playground?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=revue
-------------------------------------------------------------------------------
## Building AnswerBot with Keras and TensorFlow
With the recent advances into neural networks capabilities to process text and audio data we are very close creating a natural human assistant. TensorFlow from Google is one of the most popular neural network library, and using Keras you can simplify TensorFlow usage. TensorFlow brings amazing capabilities into natural language processing (NLP) and using deep learning, we are expecting bots to become even more smarter, closer to human experience. In this technical discussion, we will explore NLP methods in TensorFlow with Keras to create answer bot, ready to answers specific technical questions. You will learn how to use TensorFlow to train an answer bot, with specific technical questions and use various AWS services to deploy answer bot in cloud.

https://github.com/Avkash/mldl/tree/master/tensorbeat-answerbot
-------------------------------------------------------------------------------
## 6.S094: Deep Learning for Self-Driving Cars
This class is an introduction to the practice of deep learning through the applied theme of building a self-driving car. It is open to beginners and is designed for those who are new to machine learning, but it can also benefit advanced researchers in the field looking for a practical overview of deep learning methods and their application.

http://selfdrivingcars.mit.edu
-------------------------------------------------------------------------------
## Dask-SearchCV: Distributed hyperparameter optimization with Scikit-Learn
Last summer I spent some time experimenting with combining dask and scikit-learn (chronicled in this series of blog posts). The library that work produced was extremely alpha, and nothing really came out of it. Recently I picked this work up again, and am happy to say that we now have something I can be happy with. This involved a few major changes:

- A sharp reduction in scope. The previous rendition tried to implement both model and data parallelism. Not being a machine-learning expert, the data parallelism was implemented in a less-than-rigorous manner. The scope is now pared back to just implementing hyperparameter searches (model parallelism), which is something we can do well.
- Optimized graph building. Turns out when people are given the option to run grid search across a cluster, they immediately want to scale up the grid size. At the cost of more complicated code, we can handle extremely large grids (e.g. 500,000 candidates now takes seconds for the graph to build, as opposed to minutes before). It should be noted that for grids this size, an active search may perform significantly better. Relevant issue: # 29.
- Increased compatibility with Scikit-Learn. Now with only a few exceptions, the implementations of GridSearchCV and RandomizedSearchCV should be drop-ins for their scikit-learn counterparts.

http://www.kdnuggets.com/2017/05/dask-searchcv-distributed-hyperparameter-optimization-scikit-learn.html?utm_content=buffer489b5&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer
-------------------------------------------------------------------------------
## Kernelized Sorting
Object matching is a fundamental operation in data analysis. It typically requires the definition of a similarity measure between the classes of objects to be matched. Instead, we develop an approach which is able to perform matching by requiring a similarity measure only within each of the classes. This is achieved by maximizing the dependency between matched pairs of observations by means of the Hilbert Schmidt Independence Criterion. This problem can be cast as one of maximizing a quadratic assignment problem with special structure and we present a simple algorithm for finding a locally optimal solution.

http://users.sussex.ac.uk/%7Enq28/kernelized_sorting.html
-------------------------------------------------------------------------------
## Quick Draw! The Data
Over 15 million players have contributed millions of drawings playing Quick, Draw! These doodles are a unique data set that can help developers train new neural networks, help researchers see patterns in how people around the world draw, and help artists create things we haven’t begun to think of. That’s why we’re open-sourcing them, for anyone to play with.

https://quickdraw.withgoogle.com/data
