---
_Title_: PathNet: Evolution Channels Gradient Descent in Super Neural Networks

_Author(s)_: Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha‚Ä†, Andrei A. Rusu, Alexander Pritzel, Daan Wierstra; Google DeepMind

_Keywords_: giant networks, path evolution algorithm, evolution and learning, continual learning, transfer learning, multitask learning, basal ganglia

_Start_: 6/28/17

_End_:
---

1: DeepMind PathNet
==============================

## üìÉ Summary
PathNet is a new deep learning architecture that combines modular deep learning, meta-learning, and reinforcement learning. From the paper:
> For artificial general intelligence (AGI) it would be efficient if multiple users trained the same giant neural network, permitting parameter reuse, without catastrophic forgetting. PathNet is a first step in this direction. It is a neural network algorithm that uses agents embedded in the neural network whose task is to discover which parts of the network to re-use for new tasks.  

## üìù Notes
- Reuses a network consisting of many networks on multiple tasks
	- Layers of neural networks interconnected through different search methods
- Includes aspects of transfer learning, continual learning, and multitask learning. This allows the network to (in theory) continuously adapt.
- Conditional Logic
	- Programming
		- Computation
		- Conditional logic
		- Iteration/recursion
	- Neuron
		- Computational unit (sum of products)
		- Conditional unit (activation function)
		- Layers added which led to deep learning
	- Add loops into network
		- Led to RNNs
		- RNNs are chaotic so memory (buffer) is needed with led to LSTM  

## üîé Research Method
1. Read the introduction
	‚òëÔ∏è
2. Identify the big question
	Is it possible to create a neural network algorithm that identifies which parts of the network can be re-used for new tasks and achieve better results than fine-tuning and other hyperparameter optimization?
3. Summarize the background in five sentences or less
	Neural networks, in general, are trained on data for each specific task they are trying to achieve. This is time consuming and not efficient. Transfer learning was developed to bypass this problem but has limited use. PathNet seeks to combine transfer learning, continual learning, and multitask learning to solve the problem of catastrophic forgetting.
4. Identify specific questions
	How can be PathNet be implemented in order to achieve results that are better than fine-tuning and independent learning controls?
5. Identify the approach
	The general approach is as follows:
	> A PathNet is a modular deep neural network having L layers with each layer consisting of M modules. Each mod-ule is itself a neural network, here either convolutional or linear, followed by a transfer function; rectiÔ¨Åed linear units are used here. For each layer the outputs of the modules in that layer are summed before being passed into the ac-tive modules of the next layer. A module is active if it is present in the path genotype currently being evaluated (see below). A maximum of N distinct modules per layer are permitted in a pathway (typically N = 3 or 4). The Ô¨Ånal layer is unique and unshared for each task being learned. In the supervised case this is a single linear layer for each task, and in the A3C case each task (e.g. Atari game) has a value function readout and a policy readout.
	The approach for binary MNIST classification, CIFAR and SVHN, Atari games, and Labyrinth games all differed in order to accommodate each task.
6. Read the Methods section and draw a diagram of the experiment
	See figures at the end of the paper.
7. Summarize each result
	- Binary MNIST classification
		- Helps speed up learning in the classification task (mean time to solution = 167 generations versus 229 by fine tuning).
		- Learns in fewer generations than fine tuning and independent learning implementations.
		- Speedup ratio compared to independent learning was 1.18.
		- Speedup is obtained in determing when and when there shouldn't be overlap
	- CIFAR and SVHN
		- Both CIFAR and SVHN are learned faster when learned second rather than first
	- Atari games
		- Found that PathNet was superior to fine tuning in which a hyperparamter sweep was performed using learning rates and entropy costs.
		- Several hyperparamets were investigated for PathNet: evaluation time, mutation rate, and tournament size.
		- An optimal combination of tournament size and mutation rate was found for PathNet that allowed for rapid convergence.
		- Speedup of 1.33 versus 1.16 for fine tuning.
	- Labyrinth games
		- Three labyrinth games were tested, ```lt_chasm```, ```seekavoid_arena```, and ```stairway_to_melon```.
		- For fine tuning a hyperparameter sweep for mutation rates, module dupliation rates, and tournament size was used. The learning rate, entropy cost, and evaluation time were fixed.
		- PathNet learns the second task faster than fine tuning for transfer to ```lt_chasm``` and transfer from ```lt_chasm``` to ```seekavoid_arena```. PathNet also performs better when learning ```stairway_to_melon``` and ```seekavoid_arena``` from scratch.
		- Interestingly, when transferring to ```lt_chasm```, both fine tuning and PathNet perform worse than de novo learning. 
		- Speedup for fine tuning is 1.0 versus 1.26 for PathNet (this is skewed by the good performance of transferring from ```seekavoid_arena``` to ```stairway_to_melon```).
8. Do the results answer the specific questions/goals?
	Yes, they show that in most cases PathNet improves performance.
9. Read the conclusion/discussion section
	‚òëÔ∏è
10. Read the abstract
	‚òëÔ∏è
11. What do others say about this paper?
	Not too much, however, a few articles mentioning it seem to characterize the results in a very positive light.  

## üìó Resources
- [https://medium.com/intuitionmachine/pathnet-a-modular-deep-learning-architecture-for-agi-5302fcf53273][1]
- [https://medium.com/intuitionmachine/is-conditional-logic-the-new-deep-learning-hotness-96832774907b][2]
- [https://github.com/jaesik817/pathnet][3]

## üìê Project Organization
------------
    ‚îú‚îÄ‚îÄ LICENSE
    ‚îú‚îÄ‚îÄ Makefile           <- Makefile with commands like `make data` or `make train`
    ‚îú‚îÄ‚îÄ README.md          <- The top-level README for developers using this project.
    ‚îú‚îÄ‚îÄ data
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ external       <- Data from third party sources.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ interim        <- Intermediate data that has been transformed.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ processed      <- The final, canonical data sets for modeling.
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ raw            <- The original, immutable data dump.
    ‚îÇ
    ‚îú‚îÄ‚îÄ models             <- Trained and serialized models, model predictions, or model summaries
    ‚îÇ
    ‚îú‚îÄ‚îÄ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    ‚îÇ                         the creator's initials, and a short `-` delimited description, e.g.
    ‚îÇ                         `1.0-cpd-initial-data-exploration`.
    ‚îÇ
    ‚îú‚îÄ‚îÄ references         <- Data dictionaries, manuals, and all other explanatory materials.
    ‚îÇ
    ‚îú‚îÄ‚îÄ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ figures        <- Generated graphics and figures to be used in reporting
    ‚îÇ
    ‚îú‚îÄ‚îÄ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    ‚îÇ                         generated with `pip freeze > requirements.txt`
    ‚îÇ
    ‚îú‚îÄ‚îÄ src                <- Source code for use in this project.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py    <- Makes src a Python module
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ data           <- Scripts to download or generate data
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ features       <- Scripts to turn raw data into features for modeling
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ models         <- Scripts to train models and then use trained models to make
    ‚îÇ   ‚îÇ                     predictions
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ visualization  <- Scripts to create exploratory and results oriented visualizationsbre
    ‚îÇ
    ‚îî‚îÄ‚îÄ tox.ini            <- tox file with settings for running tox; see tox.testrun.org


--------

[1]:	[https://medium.com/intuitionmachine/pathnet-a-modular-deep-learning-architecture-for-agi-5302fcf53273] "PathNet - A Modular Deep Learning Architecture"
[2]:	https://medium.com/intuitionmachine/is-conditional-logic-the-new-deep-learning-hotness-96832774907b "Is Conditional Logic the New Deep Learning Hotness?"
[3]:	https://github.com/jaesik817/pathnet "PathNet on Github"

<p><small>Project based on the <a target="_blank" href="https://drivendata.github.io/cookiecutter-data-science/">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>
